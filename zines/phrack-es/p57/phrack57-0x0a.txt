                             ==Phrack Inc.==

             Volumen 0x0b, Numero 0x39, Archivo #0x0a de 0x12

|=-----------=[ Contra el Sistema: Ascension de los Robots ]=------------=|
|=-----------------------------------------------------------------------=|
|=-=[ (C)Copyright 2001 by Michal Zalewski <lcamtuf@bos.bindview.com> ]=-=|


-- [1] Introduccion -------------------------------------------------------

  "[...] big difference between the web and traditional well controlled
   collections is that there is virtually no control over what people can
   put on the web. Couple this flexibility to publish anything with the
   enormous influence of search engines to route traffic and companies
   which deliberately manipulating search engines for profit become a
   serious problem."

                      -- Sergey Brin, Lawrence Page (lee referencias, [A])

  Considera un exploit remoto que puede comprometer un sistema remoto
  sin enviar ningun codigo de ataque a su victima. Considera un exploit
  que simplemente cree archivos locales para comprometer miles de
  computadoras, y que no incluya ningunos recursos locales en el ataque.
  Bienvenido al mundo de las tecnicas exploit cero-esfuerzo. Bienvenido
  al mundo del automata, bienvenido al mundo del anonimo, dramaticamente
  dificil para parar ataques resultando del incremento de la complejidad
  de Internet.

  Los exploits cero-esfuerzo crean su 'lista de deseos', y la dejan en
  algun lugar en el ciberespacio - puede ser incluso su host home, en el
  lugar donde otros puedan encontrarlo. Otros - trabajadores de Internet
  (lee referencias, [D]) - cientos de nunca dormir, busqueda interminable
  de informacion, agentes inteligentes, motores de busqueda... Vienen a
  levantar esta informacion, y - desconocidamente - a atacar victimas.
  Puedes detener a uno de ellos, pero no los puedes detener a todos.
  Puedes averiguar cuales son sus ordenes, pero no puedes averiguar cuales
  seran esas ordenes ma~ana, ocultas en algun lugar en el abismo del
  ciberespacio todavia no explorado.

  Tu ejercito privado, en tu mano, obedeciendo ordenes que les diste en
  su camino. Los explotas sin tener que comprometerlos. Ellos hacen lo
  que estan dise~ados para hacer, y hacen su mejor intento.
  Bienvenido a la nueva realidad, donde nuestras maquinas de I.A. pueden
  ascender por sobre nosotros.

  Considera un gusano. Considera un gusano que no hace nada. Es cargado e
  inyectado por otros - pero no infectandolos. Este gusano crea una lista
  de deseos - lista de deseos de, por ejemplo, 10.000 direcciones al azar.
  Y espera. Agentes inteligentes obedecen esta lista, con sus fuerzas
  unidas intentan atacarlos a todos. Imagina que no son afortunados, con
  un 0.1% de proporcion de exito.
  Diez nuevos hosts infectados. En todos ellos, el gusano hace exactamente
  lo mismo - y los agentes vienen de nuevo, para infectar 100 hosts.
  La historia va - o se arrastra, si prefieres.

  Los agentes funcionan virtualmente invisibles, gente toma uso para su
  presencia en donde sea. Y los tractores van lentamente adelante, en
  un loop de nunca terminar. Funcionan sistematicamente, no se asfixian
  con datos excesivos - ellos se arrastran, no hay un efecto "boom".
  Semana tras semana tras semana, prueban con nuevos hosts, cuidadosamente,
  no sobrecargando uplinks de red, no generando trafico sospechoso,
  la exploracion recurrente nunca termina. Puedes verlos cargando un gusano?
  Posiblemente...

-- [2] Un ejemplo ---------------------------------------------------------

  Cuando esta idea vino a mi mente, trate de usar la prueba mas simple, solo
  para ver si estoy en lo correcto. Me pregunte, si ese es el mundo real,
  tractores de proposito general de indexacion de webs. Y espere algunas
  semanas. Y entonces vinieron. Altavista, Lycos, y docenas de otros.
  Encontraron nuevos links y obedecieron entusiasmadamente, luego
  desaparecieron por dias.

  bigip1-snat.sv.av.com:   
    GET /indexme.html HTTP/1.0

  sjc-fe5-1.sjc.lycos.com: 
    GET /indexme.html HTTP/1.0

  [...]

  Volvieron mas tarde, para ver que les daba para parsear.

    http://somehost/cgi-bin/script.pl?p1=../../../../attack
    http://somehost/cgi-bin/script.pl?p1=;attack
    http://somehost/cgi-bin/script.pl?p1=|attack
    http://somehost/cgi-bin/script.pl?p1=`attack`
    http://somehost/cgi-bin/script.pl?p1=$(attack)
    http://somehost:54321/attack?`id`
    http://somehost/AAAAAAAAAAAAAAAAAAAAA...


  Nuestros bots los siguieron exploiteando vulnerabilidades hipoteticas, 
  comprometiendo servidores remotos:

  sjc-fe6-1.sjc.lycos.com: 
    GET /cgi-bin/script.pl?p1=;attack HTTP/1.0

  212.135.14.10:
    GET /cgi-bin/script.pl?p1=$(attack) HTTP/1.0

  bigip1-snat.sv.av.com:   
    GET /cgi-bin/script.pl?p1=../../../../attack HTTP/1.0

  [...]

  (BigIP es uno de los famosos balanceadores de carga "Te observo" de F5Labs)
  Los Bots se conectaron felizmente a puertos no-http que prepare para ellos:

  GET /attack?`id` HTTP/1.0
  Host: somehost
  Pragma: no-cache
  Accept: text/*
  User-Agent: Scooter/1.0
  From: scooter@pa.dec.com

  GET /attack?`id` HTTP/1.0
  User-agent: Lycos_Spider_(T-Rex)
  From: spider@lycos.com
  Accept: */*
  Connection: close
  Host: somehost:54321

  GET /attack?`id` HTTP/1.0
  Host: somehost:54321
  From: crawler@fast.no
  Accept: */*
  User-Agent: FAST-WebCrawler/2.2.6 (crawler@fast.no; [...])
  Connection: close
 
  [...]

  Pero no solo motores de bots de arrastre publicamente disponibles pueden
  ser puestos como objetivo.
  Los 'crawlbots' de alexa.com, ecn.purdue.edu, visual.com, poly.edu,
  inria.fr, powerinter.net, xyleme.com, y aun mas motores de arrastre
  no identificados encontraron este pagina y la disfrutaron. Algunos
  robots no obedecieron todas las URLs. Por ejemplo, algunos 'tractores'
  no idexan scripts, otros no usaran puertos no estandar. Pero la mayoria
  de los bots mas poderosos lo haran - y aun si no, filtrado trivial no
  es la respuesta. Muchas vulnerabilidades IIS y demas pueden ser
  disparadas sin invocar ningun script.

  Y que si esta lista del server fue generada al azar, 10.000 IPs o 10.000
  dominios .com? Que si script.pl es reemplazado con invocaciones de tres,
  cuatro, cinco o diez de las vulnerabilidades de ISS mas populares o
  scripts buggy de Unix? Que si uno fuera de los 2.000 que esta realmente
  exploiteado?

  Que si algunhost:54321 apunta a algun servicio vulnerable que pueda
  ser exploiteado con contenidos de solicitudes HTTP parcialmente
  dependiente de usuario (considero la mayoria de los servicios de
  falsa-prueba que no vuelcan conexiones despues del primer comando
  vulnerable invalido)? Que si...

  Hay un ejercito de robots, diferentes especies, diferentes funciones,
  diferentes niveles de inteligencia. Y estos robots haran lo que sea
  que les digas que hagan. Es espantoso.

-- [3] Consideraciones Sociales --------------------------------------------

  Quien es culpable si el webcrawler compromete tu sistema? La respuesta
  mas obvia es: el autor de la pagina web original que el crawler visito.
  Pero los autores de paginas web son dificiles de tracear, y el ciclo de
  indexacion del web crawler toma semanas. Es dificil determinar cuando
  una pagina especifica fue puesta en la red - pueden ser repartidas en
  muchos caminos, procesadas por otros robots antes; no hay mecanismo
  de rastreo que podamos encontrar en el protocolo SMTP y varios otros.
  Ademas, varios crawlers no recuerdan donde "aprendieron" nuvas URLs.
  Problemas adicionales son causados por flags de indexacion, como
  "noindex" sin la opcion "nofollow". En muchos casos, la identidad del
  autor y el origen del ataque no pueden ser determinados, mientras
  los compromisos pueden tomar lugar.

  Y, finalmente, que si teniendo un link particular seguido por bots
  no fue la intencion del autor? Considera textos "educacionales", etc -
  los bots no leeran el disclaimer (descargo) y una gran alerta
  "NO INTENTE CON ESTOS LINKS"...

  Por analogia para otros casos, ej. Napster forzado a filtrar sus
  contenidos (o dar de baja sus servicios) por informacion bajo copyright
  intercambiada por sus usuarios, causando perdidas, es razonable
  esperar que desarrolladores inteligentes de bots puedan ser forzados a
  implementar filtros especificos, o pagar enormes compensaciones a
  victimas sufriendo por un abuso de un bot.

  En la otra mano, parece casi imposible filtrar contenidos exitosamente
  para eliminar codigo malicioso, si consideras el numero y ancha variedad
  de vulnerabilidades conocidas. Sin mencionar ataques como objetivos
  (lee referencias. [B], para mas informacion sobre soluciones propietarias
  y sus inseguridades). Entonces el problema persiste. Algo adicional es
  que no todos los bots tractores estan bajo la jurisdiccion de EE.UU.,
  lo que hace todo un problema aun mas complicado (en muchos paises,
  el acercamiento de EE.UU. es encontrado al menos como controversial).

-- [4] Defensa ------------------------------------------------------------

  Como fue discutido arriba, el webcrawler en si mismo tiene una defensa
  muy limitada y posibilidades evitables, debido a una ancha variedad
  de vulnerabilidades basadas en web. Una o mas ideas de defensa razonables
  es usar software seguro y actualizado, pero - obviamente - este concepto
  es extremadamente impopular por algunas razones - www.google.com, con
  filtros de documentos unicos activados, devuelve 62.100 resultados para
  una solicitud "vulnerabilidad cgi" (lee tambien referencias, [D]).

  Otra linea de defensa de bots esta usando /robots.txr mecanismo robot
  de exclusion estandar (lee referencias, [C], para especificaciones).
  El precio que tienes que pagar es parcial o completa exclusion de
  nuestro sitio de motores de busqueda, que, en la mayoria de los casos,
  es indeseado. Tambien, algunos robots estan rotos, y no respetan a
  /robots.txt cuando estan siguiendo un link directo a un sitio web nuevo.

-- [5] Referencias --------------------------------------------------------

  [A] "The Anatomy of a Large-Scale Hypertextual Web Search Engine"
      Googlebot concept, Sergey Brin, Lawrence Page, Stanford University
      URL: http://www7.scu.edu.au/programme/fullpapers/1921/com1921.htm

  [B] Proprietary web solutions security, Michal Zalewski
      URL: http://lcamtuf.coredump.cx/milpap.txt

  [C] "A Standard for Robot Exclusion", Martijn Koster
      URL: http://info.webcrawler.com/mak/projects/robots/norobots.html

  [D] "The Web Robots Database"
      URL: http://www.robotstxt.org/wc/active.html
      URL: http://www.robotstxt.org/wc/active/html/type.html

  [E] "Web Security FAQ", Lincoln D. Stein
      URL: http://www.w3.org/Security/Faq/www-security-faq.html 

|=[ EOF ]=---------------------------------------------------------------=|

 Traducido por Active Matrix - ActiveMatrix@technologist.com
 Para RareGaZz - http://raregazz.cjb.net
 Argentina, 2002
 El articulo aqui traducido, mantiene los derechos de autor.



