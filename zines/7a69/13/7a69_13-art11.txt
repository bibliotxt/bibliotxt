-=( 7A69#13 )=--=( art11 )=--=( Tecnicas Data Mining       )=--=( Memonix )=-
                              ( usadas en entornos de      )
			      ( seguridad.                 )  


                                Introduccion 
                                ____________ 

 Las tecnicas Data Mining tambien conocidas como analisis estadisticos
 automatizados tienen una gran cantidad de aplicaciones en casi cualquier
 sector existente en nuestra sociedad, por lo que tambien son de utilidad en
 cuestiones referentes a la seguridad informatica.

 Este tipo de tecnicas/conceptos son de ultima generacion en el campo de la
 estadistica, siendo de gran utilidad ya que gracias a ellas podemos ser
 capaces de predecir el comportamiento de casi cualquier 'variable', ya sea
 el comportamiento de la variable dolar en el mercado bursatil o el 
 comportamiento de la variable 'trafico icmp' en un intrusion detection
 system, y todo ello debido a que mediante estas tecnicas se pueden descubrir
 patrones ocultos en cualquier tipo de informacion aleatoria en tiempo real.

 Dentro de un entorno Data Mining podemos elegir entre varios algoritmos
 para llevar a cabo la recogida y analisis de datos, siendo los mas usuales
 y conocidos los algoritmos de regresion y clasificacion.

 1) Regresion: las tecnicas de regresion simplemente constan de la recogida
 de una cierta cantidad de datos con los que se obtiene una formula 
 matematica, a partir de la cual se pueden hacer predicciones, la unica
 necesidad que tiene esta tecnica es que necesita una considerable cantidad
 de datos suministrada de una forma continua, para que el modelo generado
 sea lo mas aproximado a la realidad, aplicandose solo a variables
 cuantitativas, es decir tamaño de los encabezamientos, IDs de los 
 fragmentos, etc.   

 2) Clasificacion: esta tecnica esta destinada para trabajar con datos no
 cuantitativos o con una mezcla de datos cuantitativos y no cuantitativos,
 siendo de utilidad en campos muy concretos.

 Para los lectores interesados en tecnicas 'avanzadas' en el campo de los IDS
 este articulo lo pueden considerar como un enfoque mas aproximado a la 
 realidad que el descrito en 'NIDS on mass parallel processing architecture'
 en el numero 57 de Phrack, ya que las tecnicas data mining estan implantadas
 en las bases de datos de Oracle o en el SQL Server 2000 de Microsoft por
 mencionar algunos productos, es decir este tipo de tecnicas no necesitan de
 ningun entorno especializado como si ocurre con las tecnicas descritas en el
 articulo anteriormente mencionado, las cuales requieren de un entorno
 mass parallel processing (MPP) o en su defecto de un emulador para maquinas
 x86.

      Debilidades intrinsecas en los modelos de comprobacion de firmas       
      ________________________________________________________________

 Como es sabido por todos en el mundo de los IDS nos podemos encontrar con
 dos tipos de modelos en terminos generales, los sistemas basados en la
 comprobacion de ciertas firmas o patrones, los cuales son indicativos de que
 el sistema esta siendo atacado o que ya ha sido comprometido, y los sistemas
 que no solo se basan en el reconocimiento de firmas de ataques conocidas, 
 sino que ademas incorporan metodos de aprendizaje... hasta aqui todo bien,
 para cualquier usuario con un nivel medio de conocimientos o necesidades 
 cualquiera de estos modelos le puede resultar lo suficientemente 'seguro' 
 como para implantarlo en su sistema... pero como no todo se reduce a una
 gama media de usuarios, en el mercado han de existir tambien otro tipo de
 sistemas basados en otros modelos, los cuales existen porque esta demostrado
 que los anteriores modelos no son del todo fiables, entendiendo por no 
 fiables que existe 'al menos' un 0.1% de posibilidades de que sean
 comprometidos; algunos se preguntaran si realmente existen tantos sistemas
 con unos requerimientos tan elevados de seguridad... si, efectivamente estos
 sistemas existen, aunque otra cosa es que en ellos se tomen las medidas
 realmente necesarias.

 Cuando he dicho que 'al menos' existe un 0.1% de posibilidades de que un
 sistema asegurado con un simple IDS basado en los anteriores modelos sea
 comprometido es por unas razones bastante simples (incluso obvias) :

 1) Los sitemas basados en los modelos de comprobacion de firmas se basan
 en el mantenimiento de grandes bases de datos de firmas de ataques conocidos
 pudiendo llegar a parar a ataques en cierto modo similares a los ya 
 recogidos en la base de datos, como se ve este modelo deja un importante 
 cabo suelto, la proteccion ante lo desconocido; es cierto que un usuario
 medio puede encontrar en este modelo su solucion ideal, pero un sistema
 critico no puede estar expuesto a un grado tan elevado de inseguridad, ya
 que el numero de habitantes del planeta Tierra que tienen acceso a exploits
 de vulnerabilidades no publicas es demasiado elevado como para dejar este
 factor en el olvido.

 2) Los sistemas que incorporan metodos de aprendizaje para protegerse ante
 la amenaza de lo desconocido son por un lado demasiado costosos como para 
 que su implantacion no suponga un problema, no solo refiriendome con ello a
 su coste monetario, sino tambien al coste que supone su mantenimiento, ya
 que un usuario sin ciertos conocimientos tecnicos seria incapaz de darle un
 uso apropiado. Por otro lado esta el grado de confianza que merecen estas
 tecnicas desarrolladas en campos como la Inteligencia Artificial, ya que no
 se han hecho las suficientes pruebas a pie de campo como para asegurar que
 estos metodos son realmente eficaces en entornos de seguridad contra las
 tegnologias de antiseguridad que estan por venir. 

 Por lo tanto cuando el responsable de seguridad de un sistema critico, el
 cual tiene alguna clase de enlace/conexion con el mundo 'exterior', se ha
 de plantear la eleccion de algun IDS, los modelos anteriormente mencionados
 no le deberian de satisfacer; entiendase por sistema critico maquinas de
 alta importancia, como puedan ser las que albergan documentos de maxima
 importancia en alguna institucion gubernamental o la maquina encargada de
 gestionar la base de datos espejo de un banco cualquiera.

 Es en este momento cuando entran en juego los sistemas basados en metodos
 estadisticos, los cuales crean un 'modelo' del comportamiento del sistema
 'normal' o 'legitimo', por lo que solo restaria comparar ese modelo creado
 con la actividad actual del sistema para encontrar cualquier tipo de
 'anormalidad', pudiendo decir que lo que vienen a hacer estos sistemas es
 'parametrizar' el sistema en el que trabajan; encontrando dentro de toda la
 variedad de sistemas que usan metodos estadisticos, los que hacen uso de las
 tecnicas conocidas como Data Mining.

                  Algoritmos Data Mining relevantes en IDS                    
                  ________________________________________

 Antes dije que los algoritmos mas comunes en los entornos Data Mining eran
 los algoritmos de regresion y clasificacion, siendo esto cierto cuando
 estamos tratando con entornos no orientados a cuestiones de seguridad, en
 nuestro caso solo nos interesan aquellos algoritmos que tengan alguna
 relacion con esta tematica, por lo que pasare a citarlos, no todos por
 supuesto, (aunque como veremos los algoritmos utiles para nuestros
 propositos son mas variados que los usados en otro tipo de entornos) :

 a) Clasificacion: este fue nombrado antes aunque fue explicado muy
 vagamente, pudiendo usar este tipo de algoritmos una vez que tenemos
 suficiente informacion sobre caracteristicas 'normales' y 'anormales' del
 sistema, pudiendo ser mas especificos si por ejemplo solo queremos
 'parametrizar' el comportamiento de un usuario o de un programa determinado,
 una vez que tenemos esos datos sobre el objeto o variable a vigilar, 
 aplicariamos sobre esos datos un algoritmo de clasificacion, que 
 determinaria futuros comportamientos normales o anormales.

 b) Analisis Secuencial: estos algoritmos son de gran utilidad, ya que
 gracias a ellos podemos 'descubrir' secuencias de eventos que muy
 frecuentemente se encuentran juntos o ligados, es decir con este tipo de
 analisis averiguamos la frecuencia de eventos relacionados con el objeto
 que estamos estudiando.

 c) Analisis de Enlaces: con estas tecnicas averiguamos la relacion existente
 entre diversos campos de la base de datos, sirviendonos para seleccionar el
 conjunto de caracteristicas mas importantes del sistema para usar estas
 mismas en nuestro sistema de deteccion de intrusos, es decir con estos
 algoritmos rebajamos en cierta medida las posibilidades de obtener falsos
 resultados en nuestros analisis, ya que nos hemos preocupado de seleccionar
 solamente aquellas variables que realmente tienen un papel destacado en
 nuestros analisis.

 d) Ripper: este es un algoritmo inductivo, que segun la informacion
 conseguida a partir de la libreria libBFD, va 'haciendose' preguntas 
 teniendo las respuestas en las caracteristicas obtenidas mediante libBFD.

 Existen mas algoritmos para este mismo proposito, pero estos requieren de
 enormes cantidades de memoria, y como el objetivo mas o menos era tratar
 con algoritmos aplicables a un entorno standard, no los mencionare, para 
 quien si quiera hacerlo le remito a 'Fast Algorithms for Association Rules'.

                      Controlando el flujo de ejecucion                         
                      _________________________________

 Como dije antes los metodos estadisticos orientados a la deteccion de
 intrusiones no solo son utilizados para controlar variables de tipo usuario,
 sino que tambien pueden ser usados para vigilar el comportamiento de un
 programa cualquiera, siendo mas logico el hacerlo con programas que corran
 como root en el sistema ya que estos son los que nos pueden dar en un futuro
 mas problemas.

 Lo unico que se requiere para llevar a cabo este tipo de tecnicas 
 estadisticas es ser capaces de monitorear la ejecucion del programa el cual
 queremos vigilar o parametrizar, lo cual podemos conseguir acumulando datos
 traceando la ejecucion normal del programa, ya que cada programa implica a
 un conjunto determinado de llamadas al sistema, pudiendo determinar pequeñas
 secuencias basandonos en el orden en que son llamadas las syscalls, con lo
 que construiriamos una base de datos que nos ayudaria a identificar 
 secuencias anormales, es decir anomalias en la ejecucion del proceso.

 Una vez que tenemos esa base de datos podemos poner en practica tecnicas
 estadisticas, como puede ser el uso del programa Ripper que nos permite
 predecir cuando una secuencia es normal o anormal, como se ve la aplicacion
 de tecnicas data mining abre un interesante camino en el mundo de la 
 seguridad, ya que la parametrizacion de los programas criticos residentes en
 nuestro sistema nos permite tener un mayor control sobre el mismo.

 Otro aspecto relacionado en cierta medida, es el uso de estas tecnicas en el
 mundo de los antivirus, ya que como ocurria en los demas casos, podemos
 detectar patrones en grandes cantidades de datos, lo cual ya se intenta
 hacer en cierta medida en las actuales tecnicas de programacion de antivirus
 ya que estos constan de un detector de firmas y de un clasificador
 heuristico para encontrar posibles nuevos virus, debiendo de decir que los
 metodos para generar estos clasificadores heuristicos no suelen ser publicos
 por lo que no se puede llegar a hacer una comparativa entre metodos
 heuristicos y metodos data mining.

 Para llevar a cabo este estudio, hemos de ser capaces de extraer ciertas
 caracteristicas de cada fichero binario, lo que nos ayudara para saber que
 es lo que puede y no puede hacer ese programa en concreto, en el caso de 
 ejecutables windows se podria usar la libreria libBFD, la cual nos dara el
 tamaño, los nombres de las DLLs usadas, los nombres de las llamadas a las
 funciones, siendo toda esta informacion extraida del fichero objeto.
 Otra opcion que se podria usar para sacar este mismo tipo de informacion es
 el uso del programa strings, siendo usado por la comunidad antivirica, pero
 se podria decir que no es del todo fiable ya que las cadenas pueden cambiar
 sin ningun tipo de problemas; por ultimo podemos hacer uso de hexdump que
 convertira archivos binarios en archivos hexadecimales, sirviendonos para
 estudiar la secuencia de bits.

 Sobre el tema de los clasificadores heuristicos hay muchas cosas de las que
 hablar ya que es un campo muy interesante, al igual que como ya he dicho es 
 un posible contrincante contra las tecnicas data mining en asuntos de 
 seguridad por lo que podemos decir algo mas sobre estos clasificadores, para
 dar una idea de sus posibles ventajas y/o desventajas; estos clasificadores
 se solian hacer a mano, siendo un trabajo bastante complejo al alcance de no
 muchas personas, siendo muy comun el hacer uso de multiples redes neuronales
 para detectar virus desconocidos; en este campo se usan tecnicas de rastreo
 muy similares a las usadas por los metodos data mining, por ejemplo Kephart
 desarrollo una tecnica para seleccionar automaticamente caracteristicas
 indicativas de la presencia de un virus, basandose en la busqueda de 
 pequeñas secuencias de bytes, proceso que tambien requiere una gran cantidad
 de ejemplares para su estudio, tanto archivos infectados como no infectados,
 siendo este campo de la deteccion automatica de nuevos virus muy tenido
 en cuenta sobre todo ultimamente, ya que el ritmo de aparicion de nuevos
 virus es muy elevado, segun el IBM High Integrity Computing Laboratory 
 aparecen casi tres nuevos virus al dia, siendo otro de los factores 
 causantes de esto el que los escaners de virus que simplemente se basen en
 una base de datos con multiples firmas, estan completamente fuera de juego,
 ya que los virus polimorficos son algo que esta a la orden del dia, donde
 virus ya conocidos pueden ser modificados en gran parte, siendo tambien
 modificada la parte correspondiente a la secuencia de bytes que componen la
 firma, y los virus que incluyen rutinas para modificar su forma 
 deliberadamente, modificando la parte principal del virus usando para ello
 una clave que es seleccionada aleatoriamente en el tiempo de replicacion,
 siendo la 'head' del virus la encargada de mutar el cuerpo del virus, donde
 en este caso la firma de tales virus se sacaria de esa misma head o cabecera
 ante lo que nos encontramos con nuevos virus capaces de generar 
 aleatoriamente diferentes cabeceras, y como fue apuntado por Fred Cohen el
 problema de distinguir un virus de un programa 'normal' es algoritmicamente
 imposible de un modo general, lo que augura un panorama no demasiado 
 alentador para la comunidad antivirica, aunque no por ello las 
 investigaciones en esta materia se van a dejar de realizar.

                          Posible Desarrollo en IDS                             
                          _________________________

 La ciencia de la Estadistica puede ayudar y mucho a la pseudociencia de la
 seguridad, existiendo una gran cantidad de tecnicas que pueden llevarse a
 este campo siendo solo cuestion de tiempo su implantacion, en cuanto a las
 tecnicas de analisis estadisticos automatizados o data mining, unos simples
 ejemplos de posibles aplicaciones pueden ser:

 - aplicar un analizador data mining a los resultados generados por TCPdump
 lo que nos permitiria detectar una gran cantidad de ataques con un 
 porcentaje bastante aceptable de falsos positivos, los ataques que se 
 podrian detectar.. denegaciones de servicio, ataques de fragmentacion,
 actividad de troyanos, la deteccion de 'estimulos' hacia nuestra maquina,
 el posible descubrimiento de 'fases' de un ataque determinado mediante la
 simple construccion de los indicadores TCP por ejemplo; en resumen se podria
 actuar sobre cualquier tipo de accion 'visible' mediante los datos que nos
 proporciona TCPdump, para lo que claro esta se requeriria que en una fase
 anterior el analizador data mining hubiese tratado con una cantidad 
 suficiente de informacion en el formato TCPdump, siendo la adecuacion de las
 reglas de analisis un paso sin mas contratiempos; lo que se trata es llevar
 al campo de la seguridad lo que ya ha sido probado en muchos otros terrenos,
 las herramientas que nos ofrece la Estadistica son absolutamente fiables y
 pocas veces se llevan a pie de campo las herramientas mas poderosas que esta
 nos proporciona, como las data mining, ya que estas son las mas complejas
 ante lo que los desarrolladores no familiarizados con las matematicas
 tienden a dejarlas en el olvido.

 - en el mundo de los ids existen lenguajes especificos, los cuales nos 
 permiten programar de una forma muy intuitiva filtros para la deteccion de
 intrusos que se limitan a detectar firmas de ataque.
 Como he dicho existen varios de estos lenguajes, siendo algunos mas 
 complicados de lo que debieran mientras que otros no son lo suficientemente
 flexibles; en mi opinion el mejor lenguaje existente para este proposito es
 el N-Code, usado en el sistema NFR, siendo muy facil tanto para escribir
 como para leer filtros, decir que la sintaxis de construccion de filtros de
 snort tampoco es demasiado dificil, unos simples ejemplos de N-Code para 
 quien nunca haya visto nada sobre el tema:

 ############################################################
 # detects if the ip direction of origin is the same that
 # the ip direction of destiny
 #
 # memonix
 ############################################################

 filter small dos()
 {
   if (ip.src == ip.dest)
	{
	  record system.time, eth.src, ip.src, eth.dst, 
	  ip.dest to land_recrdr;
	}
 }

 ############################################################
 # OOB Module (WinNuke)
 #
 # sili@l0pht.com
 ############################################################

 the_schema = library_schema:new( 1, [ "time","ip","ip" ],
	scope() );

 filter oob tcp (client, dport: 139)
 {
   $urgpointer = long(ip.blob,16);
   if ($urgpointer == 3)
	record system.time, ip.scr, ip.dst to the_recorder;
 }

 the_recorder = recorder("bin/histogram packages/test/oob.cfg",
	"the_schema");

 Despues de ver esto toca decir donde encuadramos a las tecnicas data mining,
 pues ni mas ni menos que como una posible alternativa a este tipo de
 lenguajes especificos de cada ids, la codificacion de algoritmos data 
 mining en lenguajes como C puede llegar a ser demasiado complejo cosa que
 podemos intentar solucionar si hacemos que entren en juego lenguajes de
 programacion estadisticos, como es el caso de R, que nos permitiria diseñar
 de una forma mas simple este tipo de tecnicas, siendo aun necesarias
 ciertas librerias que no estan disponibles para que esto pudiera llegar a 
 tener lugar, pero como ya digo la implementacion directa de este metodo en
 los ids haria que los falsos positivos alcanzasen minimos nunca vistos, por
 lo que el esfuerzo merece la pena.

 - los patrones de busqueda de correlaciones (tallies) que suelen ser muy
 usados en los ids, no siempre son en estos tan efectivos como se podria
 esperar, 'parametrizando' nuestro sistema o red mediante las tecnicas que
 dan nombre a este texto conseguiriamos hacerlo de una forma mucho mas
 directa; el formato TCP Quad el cual es muy usado por los analistas de ids
 ayudaria en gran medida a que esto pudiera tener lugar, esto en cierta
 medida tambien nos ayudaria a resolver los tipicos problemas de 
 almacenamiento, con lo que podriamos olvidarnos de tener que seguir los
 consejos del grupo de trabajo PDD63 sobre la validez de los analisis
 forenses tras la reduccion de datos pasado cierto tiempo.

 - ...

 Para terminar simplemente decir que el numero de tecnicas existentes para el
 tratamiento de variables aleatorias es muy extenso, teniendo todas ellas
 la importancia suficiente como para que algun dia sean llevadas al campo de 
 la seguridad informatica.

                                                                             
 Saludos a Mr.Jade y a todos los investigadores de España :)
                               
     "Se recompensa mal a un maestro si se permanece siempre discipulo"          
_____________________________________________________________________________

*EOF*
